{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Market Segmentation using Attributed Graph Community Detection\n",
    "\n",
    "**Writen by Atefeh Morsali\n",
    "\n",
    "\n",
    "\n",
    "Market segmentation is the task of dividing a market into groups of customers with homogenous needs, such that\n",
    "marketing firms can target groups and allocate resources efficiently, as customers in the same segment are likely to respond similarly to a given marketing strategy.\n",
    "\n",
    "Traditional market segmentation methods are based on clustering attribute data, such as demographics (e.g., age, gender, ethnicity) and psychographic (e.g., lifestyle, personality) profiles, using traditional clustering algorithms (e.g., k-means).\n",
    "\n",
    "Nowadays, social networks have become important in marketing, as social relationships can also impact the formation of market segments. For example, a marketing firm can use this information to design marketing campaigns that target influential users in the network. As a result, a user buys a product because one of his/her friends bought the same product.\n",
    "\n",
    "In  this  project,  we  aim  to  find  market  segments  given  social  network  data.  These  social \n",
    "relations  can  be  captured  in  a  graph  framework  where  nodes  represent  customers/users  and edges \n",
    "represent some social relationship. The properties belonging to each customer/user can be treated as node \n",
    "attributes. Hence, market segmentation becomes the problem of community detection over attributed graphs, \n",
    "where the communities are formed based on graph structure as well as attribute similarities. For this, I implemented an algorithm from the paper “Community detection based on structural and attribute similarities\" to find  the  relevant market segments and evaluated the obtained segments via influence propagation. \n",
    "\n",
    "\n",
    "In the following, I first provided a quick overview on comunity detection in general (section A), then discussed the methodology used in this project (based on paper [1]) to detect communities in attributed graph (section B). After that, I implemented a code to find relevant market segments using facebook  network  of  a  US university dataset as mentioned in section C.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Community Detection Overview\n",
    "\n",
    "A community is a set of vertices in a graph that are densely connected within each other and sparsely connected \n",
    "with the rest of the graph. Communities in social network represent social groups with vertices representing users and edges represent relationships (e.g., “friendship”) between a pair of users.\n",
    "\n",
    "\n",
    "Community detection is the problem of partitioning a given graph into communities. Solving this problem involves:\n",
    "1. Defining an  <b> objective function </b> to partition the graph into communities.\n",
    "    - <font color='blue'>Modularity</font> (the most widely used obj fnc)\n",
    "    \n",
    "    It is the difference between the number of edges within the communities and the expected number of these edges in a random graph with the same degree distribution.\n",
    "        \n",
    "        $Q = \\frac{1}{2m}\\sum \\limits _{v,w∈V}[A_{vw}-\\frac{k_{v} k_{w}}{2m}]δ(v, w)$\n",
    "        \n",
    "        where $A$ is the adjacency matrix of the graph, $m$ is the number of edges, $k_{v}$ is the degree of vertex $v$ and $δ(i,j)$ is 1 if $i$ and $j$ belong to the same community and 0 otherwise.\n",
    "\n",
    "\n",
    "\n",
    "2. Defining <b>“goodness” metrics </b> to evaluate the quality of the communities.\n",
    "    - <font color='blue'> Density </font>; the ratio of edges to the number of possible edges\n",
    "    - <font color='blue'> Conductance </font>; the fraction of edges that point outside the community\n",
    "    - <font color='blue'> Clustering coefficient </font>; the ratio of closed triplets to all triplets\n",
    "    \n",
    "    \n",
    "Optimizing modularity is an NP-complete problem, but greedy algorithms for this problem have been proposed (e.g., the Louvain method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Community Detection in Attributed Graphs\n",
    "\n",
    "Traditional community detection methods take into account only the structural information of the graph. However, many real-world networks, such as social networks, are attributed. Considering both the structural and attribute information of the graph may allow us to detect more meaningful communities.\n",
    "\n",
    "Based on paper [1], we redefine the community detection problem for attributed graphs. Solving this problem involves:\n",
    " - Defining an <b> objective function </b> to partition the attributed graph into communities.\n",
    "     - <font color='blue'> Composite modularity.\n",
    " - Defining <b> “goodness” metrics </b> to evaluate the quality of the attributed communities.\n",
    "     -  <font color='blue'> Similarity </font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Goodness metrics. </b>\n",
    "\n",
    "It needs to consider the degree of closeness of the vertices in terms of their attributes. Vertices in the same community are expected to have similar attributes. To measure attribute similarity:\n",
    " - For binary attributes, simple matching coefficient (i.e., ratio of matching attributes to all attributes) can be used .\n",
    " - For continuous attributes, similarity metric based on the Euclidean distance can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Objective Function. </b>\n",
    "\n",
    "Traditional modularity does not take into account the attribute similarity between vertices. Thus, paper [1] introduced the composite modularity, a weighted combination of modularity and similarity, as objective function for community detection in attributed graphs as follow:\n",
    "\n",
    "$Q = \\sum \\limits _{v}\\sum \\limits _{v,w∈V}α.(\\frac{1}{2m}.[A_{vw}-\\frac{k_{v} k_{w}}{2m}])  + (1 − α) . sim(v, w))$\n",
    "\n",
    "where $α$ is the weighting factor, $0 ≤ α ≤ 1$.\n",
    "\n",
    "Optimizing composite modularity is an NP-complete problem, but greedy algorithms for this problem have been proposed (e.g., the Structure-Attribute Clustering SAC1 algorithm based on the Louvain method).\n",
    "\n",
    "#### SAC1 Algorithm\n",
    "\n",
    "<font color='blue'>Input</font>: attributed graph G = (V, E, X)\n",
    "\n",
    "<font color='blue'>Output</font>: set of communities C\n",
    "\n",
    "(1) Initialize a community for each vertex v ∈ V.\n",
    "\n",
    "(2) For each vertex v ∈ V, assign v to the community that yields the highest positive gain in composite modularity.\n",
    "    - Repeat (2) until no further improvement can be achieved.\n",
    "    - Obtain a set of communities\n",
    "\n",
    "(3) Construct a new graph by aggregating the vertices in each community into a single meta-vertex.\n",
    "\n",
    "(4) Repeat (2) and (3) until no further improvement can be achieved.\n",
    "\n",
    "\n",
    "It is similar to the Louvain method except we use composite modularity in step (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Datasets\n",
    "\n",
    "The  dataset  contains  a  facebook  network  of  a  US university  (given  as  an  edgelist)  with each  node corresponding  to  a  user  profile  having  the following  attributes:  student/faculty  status,  gender,  major,\n",
    "second  major,  dorm,  and  year information. The data used in this project is downsampled to 324 users and stored in two small datasets: fb_caltech_small_edgelist.txt and fb_caltech_small_attrlist.csv.\n",
    "Unfortunately, the Facebook Data Team requested that Porter no longer distribute the dataset. It does not include \n",
    "the names of individual or even of any of the node attributes (they have been given integer ids), but Facebook seems\n",
    "to be concerned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SAC1(object):\n",
    "    def __init__(self, data_path, alpha):\n",
    "        self.data_path = data_path\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        \n",
    "        \n",
    "    def load_data(self):  #loading the queries into a list\n",
    "\n",
    "        attr_dic={} # attribute dictionary for each node\n",
    "        deg_n=[]  # degree of each node\n",
    "        num_edges=0\n",
    "        edges={} # a list of ((int,int),weight) pairs\n",
    "        edges_of_node={}\n",
    "        nodes=[]\n",
    "\n",
    "        with open(self.data_path+'/fb_caltech_small_edgelist.txt','r') as f:\n",
    "             for line in f:\n",
    "                 node,adj=line.split()\n",
    "                 if int(node) not in nodes:\n",
    "                    nodes.append(int(node))\n",
    "                 if int(adj) not in nodes:\n",
    "                    nodes.append(int(adj))\n",
    "                 try:\n",
    "                    edges[(int(node),int(adj))]+=edges[(int(node),int(adj))]\n",
    "                 except:\n",
    "                    edges[(int(node),int(adj))]=1\n",
    "\n",
    "             edges=[(k,v) for k,v in edges.items()]\n",
    "\n",
    "             deg_n=[0 for n in nodes] \n",
    "\n",
    "\n",
    "             for e in edges:\n",
    "                 num_edges +=e[1] \n",
    "                 deg_n[e[0][0]]+=e[1]\n",
    "                 deg_n[e[0][1]]+=e[1]     \n",
    "                 if e[0][0] not in edges_of_node:\n",
    "                    edges_of_node[e[0][0]]=[e]\n",
    "                 else:\n",
    "                    edges_of_node[e[0][0]].append(e) \n",
    "                 if e[0][1] not in edges_of_node:\n",
    "                    edges_of_node[e[0][1]]=[e]\n",
    "                 elif e[0][0]!=e[0][1]:\n",
    "                    edges_of_node[e[0][1]].append(e)\n",
    "\n",
    "        with open(self.data_path+'/fb_caltech_small_attrlist.csv','r') as f:\n",
    "             next(f)\n",
    "             count=0\n",
    "             for line in f:\n",
    "                 splitted_line=[int(x) for x in line.rstrip('\\n').split(',')]\n",
    "\n",
    "                 attr_dic[count]=splitted_line\n",
    "                 count +=1\n",
    "        nodes.sort()\n",
    "\n",
    "\n",
    "        return nodes,edges,edges_of_node,attr_dic,deg_n,num_edges\n",
    "\n",
    "    \n",
    "    \n",
    "    def initializing(self):\n",
    "\n",
    "        # reading the data from the given input files \n",
    "        nodes,edges, edges_of_node, attr_dic,deg_n,num_edges=self.load_data()\n",
    "        communities=[node for node in nodes]\n",
    "\n",
    "        w_n=[0 for node in nodes] #initially ther is no self loops    \n",
    "        s_att_in_com=[0 for node in nodes] # initially attribute simil is 0\n",
    "        return nodes,edges,edges_of_node,attr_dic,deg_n,num_edges,communities,w_n,s_att_in_com\n",
    "    \n",
    "    \n",
    "    def make_initial_partition(self, deg_n,nodes,edges):   # initial partition and communities\n",
    "   \n",
    "        partition=[[node] for node in nodes]\n",
    "\n",
    "        tot_deg_comm=[deg_n[node] for node in nodes]\n",
    "\n",
    "        s_in_com=[0 for n in nodes]\n",
    "        \n",
    "        for e in edges:\n",
    "            if e[0][0]==e[0][1]:\n",
    "               s_in_com[e[0][0]]+=e[1]\n",
    "               s_in_com[e[0][1]]+=e[1]\n",
    "\n",
    "        return partition,tot_deg_comm,s_in_com\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    the first phase of the algorithm\n",
    "    '''    \n",
    "    def cal_sim_attr(self, n1, n2, attr_dic):  #attribute similarity using cosine similarity\n",
    "        d1=attr_dic[n1]\n",
    "        d2=attr_dic[n2]\n",
    "        result=1-spatial.distance.cosine(d1,d2)\n",
    "        return result\n",
    "    \n",
    "    \n",
    "   \n",
    "    def calculate_modularity_gain(self, node,com, sum_G_i_nodes,num_edges,tot_deg_comm,deg_n,sim_attr):  \n",
    "        #computes the modularity of moving node in comunity.\n",
    "\n",
    "\n",
    "        delta_newman=(1/(2.0*num_edges))*(2*sum_G_i_nodes-(deg_n[node]/(2*num_edges))*tot_deg_comm[com])\n",
    "\n",
    "\n",
    "       # delta_newman=(2*sum_G_i_nodes-(deg_n[node]/(num_edges))*tot_deg_comm[com])\n",
    "\n",
    "\n",
    "        gain=self.alpha*delta_newman+(1-self.alpha)*sim_attr    \n",
    "\n",
    "        return gain\n",
    "    \n",
    "\n",
    "    def first_phase(self, nodes, edges, edges_of_node, attr_dic, deg_n, num_edges, communities,w_n,s_att_in_com):\n",
    "        # initial partition and communities\n",
    "        best_partition,tot_deg_comm,s_in_com = self.make_initial_partition(deg_n,nodes,edges)\n",
    "\n",
    "        i=0   \n",
    "        while True:\n",
    "            print (\"i= %d\"%i)\n",
    "\n",
    "            improvement=0\n",
    "            for node in nodes:\n",
    "                node_community=communities[node]\n",
    "                #the best default comminity is node itself\n",
    "                best_community=node_community\n",
    "                best_gain=0\n",
    "\n",
    "                #print node\n",
    "                node_in_part=best_partition[node_community]\n",
    "               # best_partition[node_community].remove(node) \n",
    "\n",
    "                #this is to calculate the sum of the weights within a community\n",
    "                best_shared_edges=0\n",
    "                best_shared_attr=0\n",
    "                for e in edges_of_node[node]:\n",
    "                    if e[0][0]==e[0][1]:\n",
    "                       continue\n",
    "                    if e[0][0]==node and communities[e[0][1]]==node_community or e[0][1]==node and communities[e[0][0]]==node_community:\n",
    "                       best_shared_edges+=e[1]\n",
    "\n",
    "\n",
    "                s_in_com[node_community] -=2*(best_shared_edges+w_n[node])      \n",
    "                tot_deg_comm[node_community]-=deg_n[node] \n",
    "\n",
    "                sim_attr=0          \n",
    "                #if best_partition[node_community]:\n",
    "                for n in node_in_part:\n",
    "                      # if n != node:\n",
    "                      sim_attr +=self.cal_sim_attr(n,node,attr_dic)\n",
    "                best_shared_attr=sim_attr\n",
    "                s_att_in_com[node_community]-=best_shared_attr \n",
    "\n",
    "                best_shared_attr=0\n",
    "                #communities[node]=-1\n",
    "                ncommunities={}\n",
    "\n",
    "                for node1 in nodes: \n",
    "                    community=communities[node1]\n",
    "                    if community in ncommunities:\n",
    "                       continue\n",
    "                    ncommunities[community]=1\n",
    "                    sum_G_i_nodes=0 #number of linkes between node and and members of the community, equation 4 of the paper\n",
    "                    sim_attr=0  #attribute similarity\n",
    "                    for e in edges_of_node[node]:\n",
    "                        if e[0][0]==e[0][1]:\n",
    "                           continue\n",
    "                        if e[0][0]==node and communities[e[0][1]]==community or e[0][1]==node and communities[e[0][0]]==community:\n",
    "                           sum_G_i_nodes +=e[1]\n",
    "\n",
    "                    for n in best_partition[community]:\n",
    "                       # if n!=node:\n",
    "                           sim_attr +=self.cal_sim_attr(n,node,attr_dic)\n",
    "                   #bounding Qatt by dividing by the squared size of community  \n",
    "                    sim_attr_adj=sim_attr/math.pow(len(best_partition[community]),2)\n",
    "                    #more adjusting the similarity attribute by the dividing similarity attribute to number of communities\n",
    "                    sim_attr_adj=sim_attr_adj/(len([c for c in best_partition if c]))      \n",
    "                    # compute modularity gain by moving node to the community of its neighbor\n",
    "                    gain=self.calculate_modularity_gain(node,community,sum_G_i_nodes,num_edges,tot_deg_comm,deg_n,sim_attr_adj)\n",
    "                    if gain>best_gain:\n",
    "                       best_community=community\n",
    "                       best_gain=gain\n",
    "                       best_shared_edges=sum_G_i_nodes\n",
    "                       best_shared_attr=sim_attr\n",
    "                       best_node=node1\n",
    "\n",
    "                # removing the node from its community\n",
    "                best_partition[node_community].remove(node)\n",
    "\n",
    "                #inserting node to best community\n",
    "                best_partition[best_community].append(node)\n",
    "\n",
    "                communities[node]=best_community\n",
    "                tot_deg_comm[best_community]+=deg_n[node] \n",
    "                s_in_com[best_community]+=2*(best_shared_edges+w_n[node])                \n",
    "                s_att_in_com[best_community]+=best_shared_attr\n",
    "                #by only having one improvement, improvement value becomes one and the process repeats\n",
    "                if node_community !=best_community:\n",
    "                   improvement =1\n",
    "            i+=1           \n",
    "            if not improvement or i==15: # either 15 iteration or no improvement \n",
    "\n",
    "                break\n",
    "\n",
    "        return best_partition,s_in_com,tot_deg_comm,s_att_in_com\n",
    "\n",
    "    \n",
    "                                                                                                               \n",
    "                                                                                                               \n",
    "                                                                                                              \n",
    "                                                                                                               \n",
    "    '''\n",
    "    the second phase of the algorithm\n",
    "    '''                                                                                                          \n",
    "           \n",
    "    def find_cent_com(self, p,attr_dic):  #function to find the centroid of a community: node which is most similiar to every other node\n",
    "        sim_node={}\n",
    "        for i in range(len(p)):\n",
    "            sim_node[p[i]]=0\n",
    "\n",
    "        for i in range(len(p)):\n",
    "            for j in range(len(p)):\n",
    "                sim_node[p[i]]+=self.cal_sim_attr(p[i],p[j],attr_dic)\n",
    "        return max(sim_node,key=sim_node.get)                    \n",
    "                                                                                                               \n",
    "                                                                                                              \n",
    "    def second_phase(self, communities,partition,edges,attr_dic,s_att_in_com):\n",
    "\n",
    "        old_s_att_in_com=s_att_in_com\n",
    "        s_att_in_com=[0 for com in communities]\n",
    "        new_attr_dic={} \n",
    "        # attribute within a community/partition is equal to\n",
    "        # attribute of the centroid the community\n",
    "        k=0 \n",
    "        for p in partition:\n",
    "            centroid=self.find_cent_com(p,attr_dic)\n",
    "            new_attr_dic[k]=attr_dic[centroid]\n",
    "            k+=1\n",
    "        attr_dic=new_attr_dic\n",
    "\n",
    "        old_edges=edges\n",
    "        old_communities=communities\n",
    "\n",
    "        nodes=[i for i in range(len(partition))]\n",
    "        s_att_in_com=[0 for node in nodes]\n",
    "\n",
    "\n",
    "        # relablling the communities\n",
    "        communities=[]\n",
    "        d={}\n",
    "        i=0\n",
    "        for community in old_communities:\n",
    "            if community in d:\n",
    "               communities.append(d[community])\n",
    "            else: \n",
    "                d[community]=i\n",
    "                communities.append(i)\n",
    "                i+=1\n",
    "\n",
    "        edges={}\n",
    "\n",
    "        for e in old_edges:\n",
    "            ei=communities[e[0][0]]\n",
    "            ej=communities[e[0][1]]\n",
    "            try:\n",
    "               edges[(ei,ej)]+=e[1]\n",
    "            except KeyError:\n",
    "               edges[(ei,ej)]=e[1]\n",
    "        edges=[(k,v) for k,v in edges.items()]\n",
    "\n",
    "        num=0\n",
    "        w_n=[0 for n in nodes]\n",
    "        deg_n=[0 for n in nodes]\n",
    "        edges_of_node={}\n",
    "        for e in edges:\n",
    "            num +=e[1]#this is just to check if number of edges stay the same, which must stay the same\n",
    "            deg_n[e[0][0]]+=e[1]\n",
    "            deg_n[e[0][1]]+=e[1]\n",
    "            if e[0][0]==e[0][1]:\n",
    "               w_n[e[0][0]]+=e[1]\n",
    "            if e[0][0] not in edges_of_node:\n",
    "               edges_of_node[e[0][0]]=[e]\n",
    "            else:\n",
    "               edges_of_node[e[0][0]].append(e)\n",
    "            if e[0][1] not in edges_of_node:\n",
    "               edges_of_node[e[0][1]]=[e]\n",
    "            elif e[0][0]!=e[0][1]:\n",
    "               edges_of_node[e[0][1]].append(e)\n",
    "\n",
    "        print (\"edges: %d\"%num)\n",
    "        print (\"this is a check that  no of edges must stay constant\")\n",
    "\n",
    "        communities=[n for n in nodes]\n",
    "\n",
    "        return nodes, edges, edges_of_node, deg_n, communities,w_n,attr_dic,s_att_in_com\n",
    "\n",
    "             \n",
    "                                                                                                               \n",
    "   \n",
    "    def calculate_modularity(self, partition,s_in_com,num_edges,tot_deg_comm,s_att_in_com):\n",
    "        q=0\n",
    "        m2=2.0*num_edges\n",
    "        for i in range(len(partition)):\n",
    "            q+=1/m2*s_in_com[i]-(tot_deg_comm[i]/m2)**2+s_att_in_com[i]\n",
    "        return q\n",
    "\n",
    "    \n",
    "    def louvain_method(self, nodes, edges, edges_of_node, attr_dic, deg_n, num_edges, communities,w_n,s_att_in_com):\n",
    "\n",
    "        best_partition=[]\n",
    "        actual_partition=[]\n",
    "        best_Q=-1\n",
    "        j=0\n",
    "        while True:\n",
    "            print (\"j= %d\" %j)\n",
    "\n",
    "\n",
    "            partition,s_in_com,tot_deg_comm,s_att_in_com=self.first_phase(nodes, edges, edges_of_node, attr_dic, deg_n, num_edges, communities,w_n,s_att_in_com)\n",
    "\n",
    "            Q=self.calculate_modularity(partition,s_in_com,num_edges,tot_deg_comm,s_att_in_com)\n",
    "            partition=[c for c in partition if c]\n",
    "\n",
    "            #grouping previous nodes with new partition \n",
    "\n",
    "            if actual_partition:\n",
    "               actual=[]\n",
    "               for p in partition:\n",
    "                   part=[]\n",
    "                   for n in p:\n",
    "                       part.extend(actual_partition[n])\n",
    "                   actual.append(part)\n",
    "               actual_partition=actual\n",
    "\n",
    "            else:\n",
    "               actual_partition=partition        \n",
    "\n",
    "            if Q==best_Q:\n",
    "               break\n",
    "\n",
    "            nodes,edges,edges_of_node,deg_n,communities,w_n,attr_dic,s_att_in_com=self.second_phase(communities,partition,edges,attr_dic,s_att_in_com) \n",
    "            best_partition=partition\n",
    "            best_Q=Q\n",
    "            j+=1\n",
    "    \n",
    "        return (actual_partition,best_Q) \n",
    "\n",
    "    def main(self):\n",
    "        nodes, edges, edges_of_node, attr_dic, deg_n, num_edges, communities,w_n,s_att_in_com=self.initializing()\n",
    "    \n",
    "        best_partition,best_Q=self.louvain_method(nodes, edges, edges_of_node, attr_dic, deg_n, num_edges, communities,w_n,s_att_in_com)   \n",
    "       \n",
    "        print(\"best_partition\", best_partition)\n",
    "        print (\"length of best partition %d\" %len(best_partition))\n",
    "        fw=open('communities.text','w')\n",
    "        for part in best_partition:\n",
    "            #fw.write(\"%s\\n\" %part)\n",
    "            fw.write(str(part)[1:-1]+'\\n')\n",
    "        fw.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j= 0\n",
      "i= 0\n",
      "i= 1\n",
      "i= 2\n",
      "i= 3\n",
      "edges: 6005\n",
      "this is a check that  no of edges must stay constant\n",
      "j= 1\n",
      "i= 0\n",
      "i= 1\n",
      "edges: 6005\n",
      "this is a check that  no of edges must stay constant\n",
      "j= 2\n",
      "i= 0\n",
      "edges: 6005\n",
      "this is a check that  no of edges must stay constant\n",
      "j= 3\n",
      "i= 0\n",
      "best_partition [[10, 109], [9, 22, 41, 49, 51, 91, 101, 138, 163, 173, 174, 179, 185, 243, 248, 266], [0, 2, 7, 13, 15, 16, 17, 19, 21, 24, 28, 33, 34, 36, 37, 44, 45, 47, 50, 52, 53, 54, 57, 60, 61, 64, 68, 69, 72, 73, 76, 87, 95, 96, 98, 100, 104, 106, 110, 112, 113, 119, 120, 121, 122, 125, 126, 127, 133, 135, 136, 141, 142, 144, 147, 151, 153, 154, 155, 157, 158, 159, 167, 168, 169, 177, 180, 186, 189, 190, 193, 194, 196, 198, 199, 200, 202, 203, 204, 205, 206, 209, 210, 213, 214, 218, 221, 222, 223, 229, 230, 232, 235, 237, 242, 244, 250, 252, 258, 259, 261, 268, 270, 274, 279, 280, 282, 284, 289, 292, 296, 297, 298, 308, 310, 313, 315, 316, 323], [1, 4, 5, 8, 11, 12, 18, 27, 29, 35, 38, 39, 42, 43, 48, 56, 59, 62, 65, 66, 71, 74, 75, 80, 81, 86, 88, 90, 92, 94, 99, 103, 105, 107, 108, 117, 118, 124, 128, 131, 132, 134, 139, 143, 145, 148, 150, 156, 160, 161, 162, 164, 165, 171, 175, 176, 178, 181, 182, 183, 184, 187, 188, 192, 195, 208, 212, 216, 224, 226, 227, 231, 236, 241, 245, 246, 247, 249, 251, 254, 256, 260, 263, 271, 275, 276, 277, 278, 281, 286, 287, 288, 290, 293, 295, 299, 300, 301, 302, 303, 304, 307, 309, 311, 312, 314, 320, 322, 3, 6, 14, 20, 23, 25, 26, 30, 31, 32, 40, 46, 55, 58, 63, 67, 70, 77, 78, 79, 82, 83, 84, 85, 89, 93, 97, 102, 111, 114, 115, 116, 123, 129, 130, 137, 140, 146, 149, 152, 166, 170, 172, 191, 197, 201, 207, 211, 215, 217, 219, 220, 225, 228, 233, 234, 238, 239, 240, 253, 255, 257, 262, 264, 265, 267, 269, 272, 273, 283, 285, 291, 294, 305, 306, 317, 318, 319, 321]]\n",
      "length of best partition 4\n"
     ]
    }
   ],
   "source": [
    "Sac_1 = SAC1('data', 1)\n",
    "Sac_1.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Cluster Evaluation\n",
    "\n",
    "To Evaluate the quality of the communities, I will use the concept of influence propagation. It has  been  discovered  that  by  targeting  the  influential  users/groups, desirable  marketing  goals  can  be  achieved.  Therefore,  one  way  to  evaluate  the  quality  of  the market segments (communities) is to influence an entity in each segment and measure how fast the influence propagates over the entire network. The faster that influence propagates through the entire network, the more likely an advertising campaign, for example, will be successful.\n",
    " \n",
    "The code will measure and compare the time steps taken by the influence propagation algorithm to achieve maximum propagation and compare this with kmeans clustering. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "[1] T. A. Dang, and E. Viennet. Community detection based on structural and attribute similarities. ICDS 2012.\n",
    "\n",
    "[2] CSC 591 lecture notes. Algorithm for Data Guided Business Intelligence, NCSU."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
