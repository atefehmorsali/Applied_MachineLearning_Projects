{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I will go through basic concepts of trees and how to implement basic codes in sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TREES\n",
    "1. cart (algorithm): classification and regression trees\n",
    "    - decision trees for classification\n",
    "        - to measure impurity of nodes: gini_index and cross_entropy index\n",
    "    - regression trees\n",
    "        - to measure impurity of nodes: mean_squared error and mean_absoluted error\n",
    "        \n",
    "2. Conditional Inference Trees\n",
    "    - a little bit more stable than cart\n",
    "    - select best split with correcting for multiple-hypothesis testing\n",
    "    - more fair to categorical variables\n",
    "    - only in R s far\n",
    "3. Different spliting methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Cart algorithm\n",
    "### 1. Visualizing trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.87337408, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.12662592, 0.        , 0.        ])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.tree import plot_tree\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(cancer.data, cancer.target,\\\n",
    "                                                    stratify=cancer.target, random_state = 0)\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=2)\n",
    "tree.fit(X_train, Y_train)\n",
    "tree.feature_importances_   # it says how much each feature contribute to decreasing the impurity\n",
    "\n",
    "# plot = plot_tree(tree, feature_names = cancer.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. hyperparameter tuning \n",
    "\n",
    "#### 2.1-pre_pruning: limit the size of tree while building it: tune hyperparameters: max_depth, max_leaf_nodes, min_samples_split, max_impurity_decrease, ... (in sklearn)\n",
    "- best hyperparameter: max_leaf_nodes: bc we know how much we want to split the space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/atefehmorsali/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise-deprecating',\n",
       "       estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=0,\n",
       "            splitter='best'),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'max_depth': range(1, 7)}, pre_dispatch='2*n_jobs',\n",
       "       refit=True, return_train_score='warn', scoring=None, verbose=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "hyperparam = {'max_depth':range(1,7)}\n",
    "grid = GridSearchCV(DecisionTreeClassifier(random_state=0),param_grid = hyperparam, cv=10 )\n",
    "grid.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 post_pruning: first build the tree and then shrink back its size\n",
    "- most popular method: cost complexity pruning (like regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# hyperparam = {'ccp_alpha':np.linspace(0,0.03, 20)}\n",
    "# grid = GridSearchCV(DecisionTreeClassifier(random_state=0),param_grid = hyperparam, cv=10 )\n",
    "# grid.fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "#more efficient pruning:\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "path = clf.cost_complexity_pruning_path(X_train, Y_train)\n",
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Issues and considerations in cart\n",
    "- 1. Extrapolation\n",
    "        - consider x = year, y= price; if we have a predicted linear model like y = a x + b, we can \n",
    "          extrapolate the prediction line y to predict price for upcoming years. However if we make the same\n",
    "          prediciton with regression tree, we cannot predict y for upcomming years that is bc tree always \n",
    "          compute mean on area in the training set, it will not extrapolate to new values\n",
    "        - in reality it may not be that bad, bc extrapolation is really hard but this is sth we need to keep\n",
    "          in mind\n",
    "- 2. Relation to Nearest neighbors\n",
    "    - predict average of neighbors- either by k, by epsilon ball, or by leaf  (neighbors in tree: the ones\n",
    "      in the same leaf)\n",
    "    - trees are much faster\n",
    "    - both can't extrapolate  (bc we are computing means)\n",
    "    \n",
    "- 3. Instability: tree structure is very noisy and depends how training dataset is sampled\n",
    "     - changing the random_state in DecisionTreeClassifier (from 0 to 1) -- which change how to split into \n",
    "       training and test set-- even for iris dataset( which is very simple), we will get different trees! even \n",
    "       root nodes will have completely different features.\n",
    "    - so if we build the tree and want to decides the most imp things about our dataset based on the features\n",
    "      in the root node , it is not a good idea! trees are less stable things to rely on, just split the data \n",
    "      slightly different, and we will get completely differnt root nodes and the whole structure of the tree \n",
    "      can also change\n",
    "      \n",
    "- 4. Feature importance (tree.feature_importances_): summary of structure of tree and it says how much each  \n",
    "     feature contributes to decreasing the impurity. However, this actually measures impurity decrease on the \n",
    "     training set and it's quite unstable & changes depending on the way we sample dataset; there are more \n",
    "     robust way to determine feature importance in trees like:\n",
    "      \n",
    "- 5. Trees for categorical data is not implemented in sklearn; we can do one-hot encoding and use sklearn\n",
    "\n",
    "- 6. We can use tress to predict probabilities (= fraction of class in a leaf). Without pruning all \n",
    "     probabilities are either 0 or 1 (overfitting). Even with pruning, probabilities might be too certain and \n",
    "     the tree will be still overly confident. There are ways to get good probabilities out of tress by using \n",
    "     calibration...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble methods: bagging and boosting\n",
    "Trees are very powerful but could overfit easily and they are very unstable\n",
    "- Ensemble models use wisdom of crowd by averaging a lot of models\n",
    "- Hard or soft voting; average of classes or probabilities\n",
    "     - soft voting if they are all the same type of model\n",
    "     - if one is forexample nn and the other tree, soft voting is not good bc they are not calibrated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. voting classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/atefehmorsali/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9370629370629371, 0.9370629370629371, 0.916083916083916)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#silly example, not use in actual real world, just for illustration\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "voting = VotingClassifier([('logreg', LogisticRegression(C=100)), ('tree', DecisionTreeClassifier(max_depth=3, random_state = 0))],\n",
    "                         voting = 'soft')\n",
    "voting.fit(X_train, Y_train)\n",
    "lr, tree = voting.estimators_\n",
    "voting.score(X_test, Y_test), lr.score(X_test, Y_test), tree.score(X_test, Y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Bagging (or Bootstrap Aggregation)\n",
    "- Reduce variance (avoid overfitting)\n",
    "- Bagging classifier and bagging regressor (sklearn: BaggingClassifier and BaggingRegressor)\n",
    "- Algorithm:\n",
    "    1. Create a bootstrap sample of training data (draw with replacement)\n",
    "    2. Train model on each of the trained sample dataset\n",
    "    3. Average results\n",
    "\n",
    "### 3. Variance and Bias\n",
    "Accuracy of model depend on how we sample training dataset from true data dist.\n",
    "- Variance:  it means depending on how we sample the training dataset, we get very diff prediction\n",
    "    - trees are example of high variance models\n",
    "- Bias: it means predictions are off in some sense\n",
    "    - example of model with high bias: logistic regression with nonlinear data; with nonlinear data, logistic \n",
    "      regresion will be systematically off but it will also be very consistent and will give the same results\n",
    "      always (bc they are very stable models)\n",
    "      \n",
    "- high variance models: on average correct but depends a lot on how we sample dataset.\n",
    "- high bias model are wrong\n",
    "- high bias high variance: forexample a linear regression on the problem which is not linear in a very high\n",
    "  dimensional space (in high dimensional space, linear regression is very noise without regularization, and if\n",
    "  the dataset is not linear, the model will be systematically off)\n",
    "\n",
    "\n",
    "- we want models with low variance and low bias (it means no matter how we sample the dataset, it gives the \n",
    "  same prediction and they are also very accurate on average)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
